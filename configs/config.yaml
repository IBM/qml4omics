## This is a basic config file for the qml4omics package
## It is used to run your choice of classical machine learning (CML) and quantum machine learning (QML) models on your datasets.

config_file_name: 'basic_config'

# specify output directory where input datasets are located
folder_path: 'tutorial_test_data/lower_dim_datasets'
file_dataset: 'ALL'
# or use a list as below, to only select a few datasets
# file_dataset: ['file1', 'file2', 'file3', etc]

# choose a backend for the QML methods
# backend: 'ibm_cleveland'
# backend: 'ibm_least'
backend: 'simulator'

# IBM runtime credentials - they should be in your home directory by default, but if they are elsewhere, please change path below.
qiskit_json_path: '~/.qiskit/qiskit-ibm.json'

# of ML methods to parallelize
n_jobs: 10

# select the embedding method for reducing dimensionality (number of features) in your data set
# embeddings: ['none']
embeddings: ['pca', 'nmf', 'none']

# number of dimensions (features) to embed (reduce) your data down to
n_components: 3

# ML models to generate
model: ['svc', 'dt', 'lr', 'nb', 'rf', 'mlp', 'qsvc', 'vqc', 'qnn', 'pqk']

average: 'weighted'
multi_class: 'raise'

# fix the seeds, to ensure deterministic and reproducible outcome
seed: 42
q_seed: 42
shots: 1024

# this sets the level of error mitigation for the QML methods (ranges from 1-3)
resil_level: 1

# set the ratio of train:test the data is split into, in this case 70:30
test_size: 0.3
stratify: ['y']
scaling: ['True']

# this turns on a grid search (hyperparameter tuning) for the CML methods
grid_search: False

# n-fold cross validation
cross_validation: 5
NN_depth: 1

# This sets the number of times you will perform a train-test split of your data.
# For each split, models are generated for every model-embedding combination specified above.
iter: 2

# Begin function arguments
# Each CML method will have a grid search set of arguments as well as a standard
# set of arguments to pass, which will usually be the same as the grid search arguments
# except not as a list of parameters to iterate over.
# For QML grid search, one must use generate_experiments.ipynb in tutorial_notebooks/analyses/qml_experiment_generators/,
# which will generate individual config.yaml files for each combination of parameters.
# A sample slurm script is provided, which would iterate over each config.yaml file, for use on an HPC cluster.

# begin SVC arguments
svc_args: { 'C': 0.01, 'gamma': 0.1, kernel: 'linear' }

gridsearch_svc_args: {'C': [0.1, 1, 10, 100], 
                      'gamma': [0.001, 0.01, 0.1, 1],
                      'kernel': ['linear', 'rbf', 'poly','sigmoid']
                     }

#begin DT arguments
dt_args: {'criterion': 'gini', 
          'max_depth': null,
          'min_samples_split': 2,
          'min_samples_leaf': 1,
          'max_features': null
          }

gridsearch_dt_args: {'criterion': ['gini', 'entropy', 'log_loss'], 
                     'max_depth': [null, 5, 10, 15, 20],
                     'min_samples_split': [2, 5, 10, 15],
                     'min_samples_leaf': [1, 2, 4, 6],
                     'max_features': [null, 'sqrt', 'log2']
                     }

# begin NB arguments
nb_args: {'var_smoothing': 1.e-09}

gridsearch_nb_args: {'var_smoothing': [1.e-09, 1.e-08, 1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02]}

# begin LR arguments 
lr_args: {'penalty': 'l2',
          'C': 1.0,
          'solver': 'saga',
          'max_iter':10000
          }

gridsearch_lr_args: {'penalty': ['l1', 'l2'],
                     'C': [1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03],
                     'solver': ['liblinear', 'saga'],
                     'max_iter': [5000, 10000]
                     }

# begin RF arguments
rf_args: {'n_estimators': 100,
          'max_features': 'sqrt',
          'max_depth': null,
          'min_samples_split': 2,
          'min_samples_leaf': 1,
          'bootstrap': True
          }

gridsearch_rf_args: {'n_estimators': [10, 50, 100, 200],
                     'max_features': ['sqrt', 'log2'],
                     'max_depth': [10, 20, 30, null],
                     'min_samples_split': [2, 5, 10],
                     'min_samples_leaf': [1, 2, 4],
                     'bootstrap': [True, False]
                     }

# Begin MLP arguments
mlp_args: {'hidden_layer_sizes': 100,
           'activation': 'relu', 
           'max_iter': 10000,
           'solver': 'adam',
           'alpha': 0.0001,
           'learning_rate': 'constant',
           }

gridsearch_mlp_args: {'hidden_layer_sizes': [[20,],[50,],[100,]],
                      'activation': ['tanh', 'relu'], 
                      'max_iter': [5000, 10000],
                      'solver': ['sgd', 'adam'],
                      'alpha': [0.0001, 0.05],
                      'learning_rate': ['constant','adaptive'],
                      }


#### Begin QML function parameters

# Begin QNN arguments
# add SPSA
qnn_args: {'primitive': 'estimator',
           'local_optimizer': 'COBYLA', 
           'encoding': ZZ,
           'entanglement': 'linear',
           'reps':2,
           'maxiter': 100,
           'ansatz_type' : 'amp'
           }


# Begin QSVC arguments
qsvc_args: {'C': 0.01, 
            'pegasos': False, 
            'encoding': ZZ,
            'entanglement': 'linear',
            'reps': 2,
            'primitive': 'sampler',
            }


# Begin VQC arguments
vqc_args: {'primitive': 'sampler',
           'local_optimizer': 'COBYLA', 
           'maxiter': 100,
           'encoding': ZZ,
           'entanglement': 'linear',
           'reps': 2,
           'ansatz_type' : 'amp'
           }

# Begin PRK arguments
pqk_args: {'encoding': ZZ,
           'entanglement': 'pairwise',
           'primitive': 'estimator',
           'reps': 4
           }
           
# set hydra output directories
hydra:
  run:
    dir: results/${config_file_name}/dataset=${file_dataset}/${backend}_${now:%Y-%m-%d_%H-%M-%S}
